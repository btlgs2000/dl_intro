{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_from_scratch_corso_ai.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwaFrgSW5bZBrix3HvkxbM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btlgs2000/dl_intro/blob/master/NN_from_scratch_corso_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuTb8fIQANRM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaILMw2h6SAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearLayer:\n",
        "    def __init__(self, n_in, n_out):\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "        # inizializzare con distribuzione normale standard\n",
        "        self.W = np.random.rand(n_out, n_in)\n",
        "        # inizializzare a 0\n",
        "        self.b = np.zeros(shape=(n_out, 1))\n",
        "\n",
        "        # hanno le stesse dimensioni di W e di b\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' restituisce y = Wx + b\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        x (np.array): x.shape = (n_in, 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        y (np.array): y.shape = (n_out, 1)\n",
        "        '''\n",
        "        assert x.shape == (self.n_in, 1)\n",
        "        self.x = x\n",
        "        y = self.W@x + self.b\n",
        "        assert y.shape ==(self.n_out, 1)\n",
        "        return y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        ''' restituisce dx (dl/dx) e valorizza\n",
        "        gli attributi dW e db\n",
        "        \n",
        "        args\n",
        "        ----\n",
        "        dy (np.array): dy.shape (n_out, 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        dx (np.array): dx.shape = (n_in, 1)\n",
        "        '''\n",
        "        assert dy.shape ==(self.n_out, 1)\n",
        "\n",
        "        self.db = dy\n",
        "        self.dW = dy@self.x.T\n",
        "        assert self.dW.shape == (self.n_out, self.n_in)\n",
        "        dx = self.W.T@dy\n",
        "        assert dx.shape == (self.n_in, 1)\n",
        "        return dx"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar9-Cq7KC-3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, x):\n",
        "        ''' restituisce y = sigma(x)\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        x (np.array): x.shape = (n, 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        y (np.array): x.shape = (n, 1)\n",
        "        '''\n",
        "        self.y = 1 / (1+np.exp(-x))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        ''' \n",
        "        args\n",
        "        ----\n",
        "        dy (np.array): dy.shape = (n, 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        dx (np.array): dx.shape = (n, 1)\n",
        "\n",
        "        '''\n",
        "        return self.y * (1-self.y) * dy"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlgEHem4K4wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Softmax:\n",
        "    def forward(self, x):\n",
        "        # traslazione per stabilità numerica\n",
        "        x_max = np.max(x)\n",
        "        x_reg = x - x_max\n",
        "\n",
        "        self.x_exp = np.exp(x_reg)\n",
        "        self.s = np.sum(self.x_exp)\n",
        "        assert self.s.ndim == 0\n",
        "        y = self.x_exp / self.s # normalizzazione\n",
        "        assert y.shape == x.shape\n",
        "        return y\n",
        "\n",
        "    def backward(self, dy):\n",
        "        n = dy.shape[0]\n",
        "        dx = (self.x_exp * (np.eye(n)*self.s - self.x_exp.T) / self.s**2).T @ dy\n",
        "        return dx"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6LdCkRwLKox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CrossEntropyLoss:\n",
        "    def forward(self, y_true, y_pred):\n",
        "        ''' calcola la CE\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        y_true (np.array): one-hot y_true.shape=(C, 1)\n",
        "        y_pred (np.array): è l'output della Softmax y_pred.shape=(C, 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        L (np.array): L.shape=(1, 1)\n",
        "        '''\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "        L = - np.log(y_pred[np.argmax(y_true)])\n",
        "        return L\n",
        "\n",
        "    def backward(self):\n",
        "        '''\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        dy_pred (np.array): dy_pred.shape=(C, 1)\n",
        "        '''\n",
        "        dy_pred = (-self.y_true/self.y_pred)\n",
        "        return dy_pred"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QThZqrT_fwq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers_dims, hid_layers_act, last_layer_act):\n",
        "        ''' rappresenta un Multi-layer Perceptron\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        layers_dims (List[int]): il numero di unità per ogni strato\n",
        "        hid_layers_act (str): la funzione di attivazione dopo ogni strato nascosto\n",
        "        last_layer_act (str): l'attivazione dopo l'ultimo strato\n",
        "        '''\n",
        "        self.activ_map = {\n",
        "            'sigmoid': Sigmoid,\n",
        "            'softmax': Softmax,\n",
        "            None: None\n",
        "        }\n",
        "        self.lin_layers = []\n",
        "        self.hidden_activs = []\n",
        "        self.last_activ = None\n",
        "        self.build(layers_dims, hid_layers_act, last_layer_act)\n",
        "\n",
        "    def build(self, layers_dims, hid_layers_act, last_layer_act):\n",
        "        for n_in, n_out in zip(layers_dims[:-1], layers_dims[1:]):\n",
        "            self.lin_layers.append(LinearLayer(n_in, n_out))\n",
        "        \n",
        "        for _ in self.lin_layers[:-1]:\n",
        "            self.hidden_activs.append(self.activ_map[hid_layers_act]())\n",
        "\n",
        "        self.last_activ = self.activ_map[last_layer_act]()\n",
        "\n",
        "    def forward(self, x):\n",
        "        ''' f-step\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        x (np.array): x.shape(self.layers_dims[0], 1)\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        y (np.array): y.shape(self.layers_dims[-1], 1)\n",
        "        '''\n",
        "        *hidden_layers, last_layer = self.lin_layers\n",
        "        for lin_layer, activation in zip(hidden_layers, self.hidden_activs):\n",
        "            x = lin_layer.forward(x)\n",
        "            x = activation.forward(x)\n",
        "\n",
        "        y = last_layer.forward(x)\n",
        "        if self.last_activ is not None:\n",
        "            y = self.last_activ.forward(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "        \n",
        "\n",
        "    def backward(self, dy):\n",
        "        ''' b-step\n",
        "        \n",
        "        Calcola i gradienti della loss rispetto a tutti i pesi\n",
        "\n",
        "        args\n",
        "        ----\n",
        "        dy (np.array): dy.shape(self.layers_dims[-1], 1)\n",
        "                       rappresenta dL/dy\n",
        "        '''\n",
        "        *hidden_layers, last_layer = self.lin_layers\n",
        "\n",
        "        if self.last_activ is not None:\n",
        "            dy = self.last_activ.backward(dy)\n",
        "        dy = last_layer.backward(dy)\n",
        "        for lin_layer, activation in zip(hidden_layers[::-1], self.hidden_activs[::-1]):\n",
        "            dy = activation.backward(dy)\n",
        "            dy = lin_layer.backward(dy)\n",
        "\n",
        "\n",
        "    def get_weights(self):\n",
        "        ''' restituisce una lista con i parametri della rete\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        weights (List[np.array]): lista con tutti i parametri della rete\n",
        "        '''\n",
        "        weights = []\n",
        "        for ll in self.lin_layers:\n",
        "            weights.append(ll.W)\n",
        "            weights.append(ll.b)\n",
        "        return weights\n",
        "\n",
        "    def get_gradients(self):\n",
        "        ''' dopo un passo backward restituisce la lista dei gradienti\n",
        "        nello stesso ordine della get_weights\n",
        "\n",
        "        ret\n",
        "        ---\n",
        "        gradients (List[np.array]): lista con i gradienti di tutti i parametri\n",
        "        '''\n",
        "        grads = []\n",
        "        for ll in self.lin_layers:\n",
        "            grads.append(ll.dW)\n",
        "            grads.append(ll.db)\n",
        "        return grads"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5P9i0xQ0f7O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLP(layers_dims=[64, 20, 10], hid_layers_act='sigmoid', last_layer_act='softmax')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szdK6Ab_EdX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = datasets.load_digits()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR8fHAAxSX-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs, ys = data['images'].reshape(-1, 64) / 255, OneHotEncoder(sparse=False).fit_transform(data['target'].reshape(-1, 1))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuQE_jdSSCsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "2d7bdf68-0e67-4132-ecc6-3a474c13297f"
      },
      "source": [
        "epochs = 500\n",
        "alpha = 1e-1\n",
        "ce_loss = CrossEntropyLoss()\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    losses = []\n",
        "    for x, y_true in zip(xs, ys):\n",
        "        x = x.reshape(-1, 1)\n",
        "        y_true = y_true.reshape(-1, 1)\n",
        "        # f-step\n",
        "        y_pred = mlp.forward(x)\n",
        "        loss = ce_loss.forward(y_true, y_pred)\n",
        "        losses.append(loss)\n",
        "        # b-step\n",
        "        dy = ce_loss.backward()\n",
        "        mlp.backward(dy)\n",
        "\n",
        "        # SGD step\n",
        "        weights = mlp.get_weights()\n",
        "        grads = mlp.get_gradients()\n",
        "        for weight, grad in zip(weights, grads):\n",
        "            weight[:] = weight - alpha*grad\n",
        "    pbar.set_description(f'mean loss = {np.mean(losses)}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mean loss = 0.013054869692951283: 100%|██████████| 500/500 [01:31<00:00,  5.47it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}