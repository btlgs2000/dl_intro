{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittensorflowconda4a11e7685c9d4f77b472bc84de9da134",
   "display_name": "Python 3.7.7 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import Ones, Zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormLayer(keras.layers.Layer):\n",
    "    def __init__(self, axis=-1, exp_weight_factor=0.99, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "        self.exp_weight_factor = exp_weight_factor\n",
    "\n",
    "    def reshape_weights(self, weights):\n",
    "        return [tf.reshape(w, shape=self.weights_shape) for w in weights]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_shape = [1]*len(input_shape)\n",
    "        self.w_dim = input_shape[self.axis]\n",
    "        self.weights_shape[self.axis] = self.w_dim\n",
    "        self.axis = self.axis if self.axis >= 0 else len(input_shape)-1\n",
    "        self.reduce_axes = [i for i in range(len(input_shape)) if i != self.axis]\n",
    "        self.alpha = self.add_weight(shape=(self.w_dim,), initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(shape=(self.w_dim,), initializer=Zeros(), trainable=True)\n",
    "        self.sigma = self.add_weight(shape=(self.w_dim,), initializer=Ones(), trainable=False)\n",
    "        self.mu = self.add_weight(shape=(self.w_dim,), initializer=Zeros(), trainable=False)\n",
    "\n",
    "    def _update_params(self, inputs):\n",
    "        w = self.exp_weight_factor\n",
    "        mu_batch = tf.math.reduce_mean(inputs, axis=self.reduce_axes)\n",
    "        sigma_batch = tf.math.reduce_std(inputs, axis=self.reduce_axes)\n",
    "        self.mu.assign(w*self.mu + (1-w)*mu_batch)\n",
    "        self.sigma.assign(w*self.sigma + (1-w)*sigma_batch)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        alpha, beta, sigma, mu = self.reshape_weights([self.alpha, self.beta, self.sigma, self.mu])\n",
    "        if training:\n",
    "            self._update_params(inputs)\n",
    "        norm = (inputs-mu) / sigma\n",
    "        res = (norm*alpha) + beta\n",
    "        return res\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'axis': self.axis, 'exp_weight_factor': exp_weight_factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNormLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TensorShape([32, 10])"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(32, 10).astype(np.float32)*10 +7\n",
    "out = bn(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    out = bn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.85505694,\n 0.84585935,\n 0.76592386,\n 0.81223124,\n 0.62168145,\n 0.8850148,\n 0.8502795,\n 1.1466483,\n 0.6224386,\n 1.1259699,\n 1.0906398,\n 0.887234,\n 1.073935,\n 1.0827506,\n 0.74335426,\n 0.86408806,\n 0.8345361,\n 1.0506405,\n 0.5948313,\n 1.2931309,\n 0.9339112,\n 1.0924976,\n 0.9098994,\n 1.2603775,\n 0.830108,\n 0.69927573,\n 1.2257032,\n 0.9891211,\n 0.73111165,\n 0.85022646,\n 1.3250093,\n 0.94862163]"
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "source": [
    "[np.std(out[i]) for i in range(out.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroputLayer(keras.layers.Layer):\n",
    "    def __init__(self, keep_prob, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.keep_prob = keep_prob\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'keep_prob': self.keep_prob}"
   ]
  }
 ]
}